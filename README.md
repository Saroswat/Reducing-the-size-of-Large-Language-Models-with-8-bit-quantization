# Reducing-the-size-of-Large-Language-Models-with-8-bit-quantization